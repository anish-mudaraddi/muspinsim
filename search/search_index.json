{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MuSpinSim's documentation MuSpinSim is a program designed to carry out spin dynamics calculations for muon science experiments. MuSpinSim can: simulate zero, transverse and longitudinal field experiments simulate experiments resolved in time, field, or temperature include the effects of hyperfine, dipolar, quadrupolar and Zeeman couplings simulate quantum systems exchanging energy with the environment with the Lindblad master equation fit experimental data with simulations using all of the above run in parallel on multiple cores for the most expensive tasks How to install Download the latest version of the code from Github and unzip it, then in the command line enter the folder and use pip to install it: $> pip install ./ It can then be run from anywhere simply with $> muspinsim input_file Topics Learn about the theory of spin dynamics behind MuSpinSim","title":"Welcome to MuSpinSim's documentation"},{"location":"#welcome-to-muspinsims-documentation","text":"MuSpinSim is a program designed to carry out spin dynamics calculations for muon science experiments. MuSpinSim can: simulate zero, transverse and longitudinal field experiments simulate experiments resolved in time, field, or temperature include the effects of hyperfine, dipolar, quadrupolar and Zeeman couplings simulate quantum systems exchanging energy with the environment with the Lindblad master equation fit experimental data with simulations using all of the above run in parallel on multiple cores for the most expensive tasks","title":"Welcome to MuSpinSim's documentation"},{"location":"#how-to-install","text":"Download the latest version of the code from Github and unzip it, then in the command line enter the folder and use pip to install it: $> pip install ./ It can then be run from anywhere simply with $> muspinsim input_file","title":"How to install"},{"location":"#topics","text":"Learn about the theory of spin dynamics behind MuSpinSim","title":"Topics"},{"location":"hamiltonian/","text":"Hamiltonian In this section we will go a bit more in detail into the general Hamiltonian for a system in MuSpinSim, each term, and how they are defined and implemented. The generic Hamiltonian for any MuSpinSim simulation can be written as follows: \\[ \\mathcal{H} = \\mathcal{H}_Z + \\mathcal{H}_{hfine} + \\mathcal{H}_{dip} + \\mathcal{H}_Q \\] Where: \\(\\mathcal{H}_Z\\) is the Zeeman Hamiltonian \\(\\mathcal{H}_{hfine}\\) is the hyperfine Hamiltonian \\(\\mathcal{H}_{dip}\\) is the dipolar Hamiltonian \\(\\mathcal{H}_Q\\) is the quadrupolar Hamiltonian Let's now look at these more in detail. For developers: the system Hamiltonian is built by the SpinSystem class, found in muspinsim/spinsys.py . While here we detail the terms in energy units, in practice MuSpinSim internally uses Hamiltonians in frequency units, specifically, MHz. This tends to be a very natural choice of unit for these problems as it coincides with the units used for hyperfine tensors, and keeps the numbers small and manageable, minimising risks of over or underflow. Working in SI units, this would mean that to get them in Joule you'd have to multiply by a factor of \\(h\\cdot 10^6\\) . Zeeman Hamiltonian The Zeeman interaction is the interaction of any spin with an external magnetic field. In MuSpinSim this mainly means a strong global field applied to the entire experiment, though it is possible for the user to also set custom local magnetic fields for individual spins (representing for example the effect of nearby paramagnetic nuclei). The Hamiltonian written explicitly is: \\[ \\mathcal{H}_Z = 2\\pi\\hbar\\sum_i^N \\gamma_i (\\mathbf{B}+\\mathbf{B}_i)\\mathbf{S}_i \\] The sum is intended over all spins, with the gyromagnetic ratios \\(\\gamma_i\\) depending on the nuclei's own properties. The magnetic field here was split in two parts, a global and a local one which bears the index of the spin. Of course, both of these can be zero, and by default are if the user does not specify anything. \\(\\mathbf{S}_i\\) is the vector of spin operators. For developers: SpinSystem has an add_zeeman_term() method to deal with this term. Hyperfine Hamiltonian The hyperfine interaction is often the main interaction we care about in muon spin resonance simulations. It represents an interaction between an electronic spin and a nuclear one (muon or otherwise). While physically it is often distinguished in two terms - a Fermi contact term due to the electronic spin density at the site of the nucleus as well as a dipolar part at a distance - these are effectively both incorporated in a single \\(3 \\times 3\\) symmetric tensor with non-zero trace. The following interaction between spins is written like this: \\[ \\mathcal{H}_{hfine} = 2\\pi\\hbar \\sum_{i < j}^N \\mathbf{S}_i\\mathbf{A}_{ij}\\mathbf{S}_j \\] (assuming a hyperfine tensor \\(\\mathbf{A}_{ij}\\) in frequency units). MuSpinSim also makes sure that hyperfine terms can only be defined when one of the two spins is confirmed to be an electron. For developers: SpinSystem has an add_hyperfine_term() method to deal with this term. Dipolar Hamiltonian Dipole-dipole interactions are the result of one spin interacting with the magnetic field generated by a different one. Conceptually, they are not unlike hyperfine interactions. The differences between them are due to the fact that unlike with a muon or nucleus and an electron, both dipoles in this case are considered point-like. This leads to two consequences: there is no Fermi contact term, as the two spins don't overlap; therefore the tensor is traceless; both spins can be approximated as being localised at precise points, which gives a very well defined geometric formula for the interaction tensor between them that only depends on the vector connecting them. Given two spins \\(i\\) and \\(j\\) , and a vector connecting them \\(\\mathbf{r}_{ij}\\) , the dipolar tensor for them can be computed as: \\[ \\mathbf{D}_{ij} = -\\frac{\\mu_0\\hbar \\gamma_i \\gamma_j}{2|r_{ij}|^3}\\left(\\frac{3}{|r_{ij}|^2}\\mathbf{r}_{ij}\\otimes \\mathbf{r}_{ij} - \\mathbb{I} \\right) \\] This will return a tensor in frequency units, and thus the dipolar Hamiltonian can be defined like the hyperfine one: \\[ \\mathcal{H}_{dip} = 2\\pi\\hbar \\sum_{i < j}^N \\mathbf{S}_i\\mathbf{D}_{ij}\\mathbf{S}_j \\] Quadrupolar Hamiltonian Nuclei with spin greater than \u00bd can possess a quadrupole moment . This can be roughly understood in classical terms as the result of the nuclei's internal charge distribution (due to proton and neutron arrangement) not being perfectly symmetric, and the resulting inhomogeneities in the electrostatic field. Due to having spin 1/2, neither electrons, nor muons, nor protons possess a quadrupole moment. However, some other nuclei (most commonly, \\(^{14}N\\) ) do possess a quadrupole moment. In presence of an electric field gradient (EFG), this affects the energies of their spin levels through quadrupolar self-interaction. This can then indirectly affect the results of a muon experiment when the particle interacts with the quadrupolar nucleus either through dipolar or hyperfine coupling. For this reason, MuSpinSim includes the possibility of simulating quadrupolar interactions as well. The Hamiltonian term for quadrupolar interactions depends mainly on the EFG, which can be written as a \\(3 \\times 3\\) symmetric, traceless tensor: \\[ \\mathbf{Z} = \\begin{bmatrix} \\frac{\\partial E_x}{\\partial x} & \\frac{\\partial E_x}{\\partial y} & \\frac{\\partial E_x}{\\partial z} \\\\ \\frac{\\partial E_y}{\\partial x} & \\frac{\\partial E_y}{\\partial y} & \\frac{\\partial E_y}{\\partial z} \\\\ \\frac{\\partial E_z}{\\partial x} & \\frac{\\partial E_z}{\\partial y} & \\frac{\\partial E_z}{\\partial z} \\end{bmatrix} \\] The quadrupolar interaction Hamiltonian is then found as: $$ \\mathcal{H} Q = \\sum {S_i > 1/2}^N \\frac{eQ_i}{2S_i(2S_i-1)}\\mathbf{S}_i\\mathbf{Z}_i\\mathbf{S}_i $$ where \\(e\\) is the elementary charge, \\(Q_i\\) the quadrupolar moment of the nucleus (these are tabulated, and MuSpinSim has data for most relevant isotopes of the periodic table) and \\(S_i\\) the spin of the nucleus.","title":"Hamiltonian"},{"location":"hamiltonian/#hamiltonian","text":"In this section we will go a bit more in detail into the general Hamiltonian for a system in MuSpinSim, each term, and how they are defined and implemented. The generic Hamiltonian for any MuSpinSim simulation can be written as follows: \\[ \\mathcal{H} = \\mathcal{H}_Z + \\mathcal{H}_{hfine} + \\mathcal{H}_{dip} + \\mathcal{H}_Q \\] Where: \\(\\mathcal{H}_Z\\) is the Zeeman Hamiltonian \\(\\mathcal{H}_{hfine}\\) is the hyperfine Hamiltonian \\(\\mathcal{H}_{dip}\\) is the dipolar Hamiltonian \\(\\mathcal{H}_Q\\) is the quadrupolar Hamiltonian Let's now look at these more in detail. For developers: the system Hamiltonian is built by the SpinSystem class, found in muspinsim/spinsys.py . While here we detail the terms in energy units, in practice MuSpinSim internally uses Hamiltonians in frequency units, specifically, MHz. This tends to be a very natural choice of unit for these problems as it coincides with the units used for hyperfine tensors, and keeps the numbers small and manageable, minimising risks of over or underflow. Working in SI units, this would mean that to get them in Joule you'd have to multiply by a factor of \\(h\\cdot 10^6\\) .","title":"Hamiltonian"},{"location":"hamiltonian/#zeeman-hamiltonian","text":"The Zeeman interaction is the interaction of any spin with an external magnetic field. In MuSpinSim this mainly means a strong global field applied to the entire experiment, though it is possible for the user to also set custom local magnetic fields for individual spins (representing for example the effect of nearby paramagnetic nuclei). The Hamiltonian written explicitly is: \\[ \\mathcal{H}_Z = 2\\pi\\hbar\\sum_i^N \\gamma_i (\\mathbf{B}+\\mathbf{B}_i)\\mathbf{S}_i \\] The sum is intended over all spins, with the gyromagnetic ratios \\(\\gamma_i\\) depending on the nuclei's own properties. The magnetic field here was split in two parts, a global and a local one which bears the index of the spin. Of course, both of these can be zero, and by default are if the user does not specify anything. \\(\\mathbf{S}_i\\) is the vector of spin operators. For developers: SpinSystem has an add_zeeman_term() method to deal with this term.","title":"Zeeman Hamiltonian"},{"location":"hamiltonian/#hyperfine-hamiltonian","text":"The hyperfine interaction is often the main interaction we care about in muon spin resonance simulations. It represents an interaction between an electronic spin and a nuclear one (muon or otherwise). While physically it is often distinguished in two terms - a Fermi contact term due to the electronic spin density at the site of the nucleus as well as a dipolar part at a distance - these are effectively both incorporated in a single \\(3 \\times 3\\) symmetric tensor with non-zero trace. The following interaction between spins is written like this: \\[ \\mathcal{H}_{hfine} = 2\\pi\\hbar \\sum_{i < j}^N \\mathbf{S}_i\\mathbf{A}_{ij}\\mathbf{S}_j \\] (assuming a hyperfine tensor \\(\\mathbf{A}_{ij}\\) in frequency units). MuSpinSim also makes sure that hyperfine terms can only be defined when one of the two spins is confirmed to be an electron. For developers: SpinSystem has an add_hyperfine_term() method to deal with this term.","title":"Hyperfine Hamiltonian"},{"location":"hamiltonian/#dipolar-hamiltonian","text":"Dipole-dipole interactions are the result of one spin interacting with the magnetic field generated by a different one. Conceptually, they are not unlike hyperfine interactions. The differences between them are due to the fact that unlike with a muon or nucleus and an electron, both dipoles in this case are considered point-like. This leads to two consequences: there is no Fermi contact term, as the two spins don't overlap; therefore the tensor is traceless; both spins can be approximated as being localised at precise points, which gives a very well defined geometric formula for the interaction tensor between them that only depends on the vector connecting them. Given two spins \\(i\\) and \\(j\\) , and a vector connecting them \\(\\mathbf{r}_{ij}\\) , the dipolar tensor for them can be computed as: \\[ \\mathbf{D}_{ij} = -\\frac{\\mu_0\\hbar \\gamma_i \\gamma_j}{2|r_{ij}|^3}\\left(\\frac{3}{|r_{ij}|^2}\\mathbf{r}_{ij}\\otimes \\mathbf{r}_{ij} - \\mathbb{I} \\right) \\] This will return a tensor in frequency units, and thus the dipolar Hamiltonian can be defined like the hyperfine one: \\[ \\mathcal{H}_{dip} = 2\\pi\\hbar \\sum_{i < j}^N \\mathbf{S}_i\\mathbf{D}_{ij}\\mathbf{S}_j \\]","title":"Dipolar Hamiltonian"},{"location":"hamiltonian/#quadrupolar-hamiltonian","text":"Nuclei with spin greater than \u00bd can possess a quadrupole moment . This can be roughly understood in classical terms as the result of the nuclei's internal charge distribution (due to proton and neutron arrangement) not being perfectly symmetric, and the resulting inhomogeneities in the electrostatic field. Due to having spin 1/2, neither electrons, nor muons, nor protons possess a quadrupole moment. However, some other nuclei (most commonly, \\(^{14}N\\) ) do possess a quadrupole moment. In presence of an electric field gradient (EFG), this affects the energies of their spin levels through quadrupolar self-interaction. This can then indirectly affect the results of a muon experiment when the particle interacts with the quadrupolar nucleus either through dipolar or hyperfine coupling. For this reason, MuSpinSim includes the possibility of simulating quadrupolar interactions as well. The Hamiltonian term for quadrupolar interactions depends mainly on the EFG, which can be written as a \\(3 \\times 3\\) symmetric, traceless tensor: \\[ \\mathbf{Z} = \\begin{bmatrix} \\frac{\\partial E_x}{\\partial x} & \\frac{\\partial E_x}{\\partial y} & \\frac{\\partial E_x}{\\partial z} \\\\ \\frac{\\partial E_y}{\\partial x} & \\frac{\\partial E_y}{\\partial y} & \\frac{\\partial E_y}{\\partial z} \\\\ \\frac{\\partial E_z}{\\partial x} & \\frac{\\partial E_z}{\\partial y} & \\frac{\\partial E_z}{\\partial z} \\end{bmatrix} \\] The quadrupolar interaction Hamiltonian is then found as: $$ \\mathcal{H} Q = \\sum {S_i > 1/2}^N \\frac{eQ_i}{2S_i(2S_i-1)}\\mathbf{S}_i\\mathbf{Z}_i\\mathbf{S}_i $$ where \\(e\\) is the elementary charge, \\(Q_i\\) the quadrupolar moment of the nucleus (these are tabulated, and MuSpinSim has data for most relevant isotopes of the periodic table) and \\(S_i\\) the spin of the nucleus.","title":"Quadrupolar Hamiltonian"},{"location":"theory_1/","text":"Theory of spin dynamics - I Introduction Spin is an essentially quantum mechanical phenomenon. While single spins can sometimes be usefully visualised in a classical way, as dipoles with a definite direction that are subject to rotation by precession under applied fields, this classical description quickly breaks down when multiple coupled spins are involved - and that is the premise of any and all mildly interesting muon science experiments. So, there's very little way out of the fact that muon experiments have to be described with quantum mechanical equations. Here we're going to give a quick overview of the relevant equations, their meaning, and the matrix formalism that is used to implement them numerically in MuSpinSim. The basics: spin states and Hamiltonians Spin states as vectors Quantum mechanics is often taught first with an eye to particles, like electrons, which are described by a complex-valued wavefunction \\(\\psi(x)\\) all across the three-dimensional space. The value of the wave function at a point corresponds to the amplitude of finding the particle at that point. Quantum amplitudes act like probabilities, in the sense that their square modulus \\(\\psi^*\\psi\\) expresses the probability of finding the particle at that specific point, but being complex numbers have the peculiar property that they can interfere with themselves constructively or destructively, leading to many of quantum mechanics' most counter-intuitive results (such as the way an electron's probability amplitudes add up to fringes in the double slit experiment). When it comes to individual spins, the situation is not that different, except for the fact that the wavefunction is not defined over an infinite amount of points in \\(\\mathbb{R}^3\\) ; instead, it is defined over a discrete number of states that the spin can occupy. Specifically, for a particle with spin \\(S\\) , there are \\(2S+1\\) possible states. In the simplest case, a spin-\u00bd particle, there are just two possible states: up, \\(\\mid\\uparrow\\rangle\\) and down, \\(\\mid\\downarrow\\rangle\\) . For this reason, a spin-\u00bd particle can also be considered a \"quantum bit\", or qubit . Electrons, muons and protons (namely, hydrogen nuclei) are all spin-\u00bd particles. The wavefunction of a spin-\u00bd particle can therefore be expressed with just two complex coefficients: \\[ \\mid\\psi_{1/2}\\rangle = a\\mid\\uparrow\\rangle + b\\mid\\downarrow\\rangle \\] with \\(a^*a+b^*b = 1\\) as a normalisation condition. One possible convention to write this wavefunction and manipulate it in a computer program is to treat these states as the basis for a vector space (which effectively, they are: they obey the same inner product rules as the versors for a regular Euclidean 2D space). Then we can write the wavefunction as a column vector: \\[ \\mid\\uparrow\\rangle = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\qquad \\mid\\downarrow\\rangle = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\qquad \\mid\\psi_{1/2}\\rangle = \\begin{bmatrix} a \\\\ b \\end{bmatrix} \\] Conversely, we can write the complex conjugate versions of these states (the \"bras\" to these \"kets\") as row vectors: \\[ \\langle\\uparrow\\mid = \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\qquad \\langle\\downarrow\\mid = \\begin{bmatrix} 0 & 1 \\end{bmatrix} \\qquad \\langle\\psi_{1/2}\\mid = \\begin{bmatrix} a^* & b^* \\end{bmatrix} \\] (where of course it's important to remember that the coefficients too are to be conjugated). That way, one can see for example how the inner product results naturally as a scalar product between vectors. Operators as matrices If we are describing spin states as vectors, operators can be described as matrices instead. In particular, for the spin-\u00bd particle, we have the Pauli matrices: \\[ S_x = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\qquad S_y = \\begin{bmatrix} 0 & -i \\\\ i & 0 \\end{bmatrix} \\qquad S_z = \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix} \\] for the three spatial components of the spin. Together with the identity matrix, these form a complete basis for the operators for this kind of spin. These operators can be easily used to compute expectation values. For example, consider the following state: \\[ \\psi = \\begin{bmatrix} \\frac{\\sqrt{3}}{2} \\\\ \\frac{1}{2}i \\end{bmatrix} \\] We can find the expectation values of its components by applying simple matrix product rules: \\[ \\langle S_x \\rangle = \\langle\\psi\\mid S_z \\mid\\psi\\rangle = \\begin{bmatrix} \\frac{\\sqrt{3}}{2} & -\\frac{1}{2}i \\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\begin{bmatrix} \\frac{\\sqrt{3}}{2} \\\\ \\frac{1}{2}i \\end{bmatrix} = \\begin{bmatrix} \\frac{\\sqrt{3}}{2} & -\\frac{1}{2}i \\end{bmatrix} \\begin{bmatrix} \\frac{1}{2}i \\\\ \\frac{\\sqrt{3}}{2} \\end{bmatrix} = 0 \\] \\[ \\langle S_y \\rangle = \\langle\\psi\\mid S_z \\mid\\psi\\rangle = \\begin{bmatrix} \\frac{\\sqrt{3}}{2} & -\\frac{1}{2}i \\end{bmatrix} \\begin{bmatrix} 0 & -i \\\\ i & 0 \\end{bmatrix} \\begin{bmatrix} \\frac{\\sqrt{3}}{2} \\\\ \\frac{1}{2}i \\end{bmatrix} = \\begin{bmatrix} \\frac{\\sqrt{3}}{2} & -\\frac{1}{2}i \\end{bmatrix} \\begin{bmatrix} \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2}i \\end{bmatrix} = \\frac{\\sqrt{3}}{2} \\] \\[ \\langle S_z \\rangle = \\langle\\psi\\mid S_z \\mid\\psi\\rangle = \\begin{bmatrix} \\frac{\\sqrt{3}}{2} & -\\frac{1}{2}i \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix} \\begin{bmatrix} \\frac{\\sqrt{3}}{2} \\\\ \\frac{1}{2}i \\end{bmatrix} = \\begin{bmatrix} \\frac{\\sqrt{3}}{2} & -\\frac{1}{2}i \\end{bmatrix} \\begin{bmatrix} \\frac{\\sqrt{3}}{2} \\\\ -\\frac{1}{2}i \\end{bmatrix} = \\frac{1}{2} \\] We can learn a few things from it. The three expectation values along x, y, z would correspond, classically, to the three components of the spin's magnetic moment. In classical terms, this would be a vector making a \\(60\u00b0\\) angle with the vertical, pointing towards the y direction. In general this example shows two important features of spin-\u00bd states: the component along z of the moment is determined by the relative probabilities of finding the spin up or down. If the probability are equal, the component along z is zero; the component in the xy plane can only be non-zero if the spin exists in some mixture of up and down states, and it's maximum if the up and down states have amplitudes with the same modulus. The specific direction of the component is then controlled by the relative phase of those amplitudes. For developers : quantum operators are defined by the class Operator and derived classes in muspinsim/spinop.py . Hamiltonian and time evolution Hamiltonians are operators with the additional required property of being Hermitian , that is, they have to be identical to their conjugate transpose. For a spin system, the Hamiltonian is the operator whose expectation value is the energy of that system. The Hamiltonian also controls the time evolution of the system; like for every other quantum system, the dynamics of a spin system are defined by the equation \\[ H\\mid\\psi\\rangle = i\\hbar\\frac{\\partial}{\\partial t}\\mid\\psi\\rangle, \\] whose solution is \\[ \\mid \\psi(t) \\rangle = e^{-\\frac{i}{\\hbar}Ht} \\mid \\psi(0) \\rangle. \\] The key task of MuSpinSim is to solve this equation in time, and then estimate the expectation values of the observables that interest us. The simplest way to do so in the case of a small system is to diagonalise the Hamiltonian, which numerically can be done fairly easily by taking advantage of its properties (for example using the NumPy routine numpy.linalg.eigh ). This gives a number of eigenvalues \\(\\lambda_i\\) and corresponding eigenvectors (namely, eigenstates) \\(\\mid u_i \\rangle\\) . One can then write the Hamiltonian matrix as: $$ H = \\sum_i \\lambda_i \\mid u_i \\rangle \\langle u_i \\mid = UH_0U^\\dagger $$ where \\(H_0\\) is a diagonal matrix with the eigenvalues along its diagonal, and \\(U\\) is the matrix with the eigenvectors for columns (and \\(U^\\dagger\\) its conjugate transpose). One can then transform the wavefunction in this new basis, and the matrix exponential of the now diagonal Hamiltonian becomes trivial. For developers: the Hamiltonian class is a mixin inheriting from Operator and Hermitian and is found in muspinsim/hamiltonian.py . The density matrix formalism Until here we've focused on wave functions as a way to write quantum states. However, in practice, in MuSpinSim we never use simple wave functions to express the state of a system - rather, we use density matrices . The density matrix formalism is a generalisation of the state vectors we described above that allows us to describe statistical ensembles of quantum states, rather than just individual pure states. Density matrices are especially useful and important when dealing with spin systems at a thermal equilibrium, which is what makes them so essential in MuSpinSim, as any spin other than the muon itself is usually in a thermal state at the beginning of the experiment. The density matrix is an operator whose expectaction value with a certain state is the probability to find the system in that state. For a pure state in a spin-\u00bd system as the one described above, the density matrix would be \\[ \\rho = a^*a \\mid \\uparrow \\rangle \\langle \\uparrow \\mid + a^*b \\mid \\downarrow \\rangle \\langle \\uparrow \\mid + b^*a \\mid \\uparrow \\rangle \\langle \\downarrow \\mid + b^*b \\mid \\downarrow \\rangle \\langle \\downarrow \\mid = \\begin{bmatrix} a \\\\ b \\end{bmatrix} \\begin{bmatrix} a^* & b^* \\end{bmatrix} = \\begin{bmatrix} a^*a & b^*a \\\\ a^*b & b^*b \\end{bmatrix}. \\] This type of product between vectors is called the outer product . One can see how the normalisation rule implies that \\(\\mathrm{Tr}(\\rho) = 1\\) . The expectation value of an operator can be found \\[ \\langle O \\rangle = \\mathrm{Tr}(\\rho O) \\] and the time evolution is \\[ \\frac{\\partial \\rho}{\\partial t} = -\\frac{i}{\\hbar}[H, \\rho] \\qquad\\implies\\qquad \\rho(t) = e^{-\\frac{i}{\\hbar}Ht}\\rho(0)e^{\\frac{i}{\\hbar}Ht}. \\] To see an example of the usefulness of density matrices, let's consider again a spin-\u00bd example. Consider an ensemble of particles prepared such that half of them is prepared in a state \\(\\psi_+\\) and the other half in a state \\(\\psi_-\\) : \\[ \\mid\\psi_+\\rangle = \\begin{bmatrix} \\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\end{bmatrix} \\qquad \\mid\\psi_-\\rangle = \\begin{bmatrix} \\frac{\\sqrt{2}}{2} \\\\ \\frac{-\\sqrt{2}}{2} \\end{bmatrix}. \\] Now imagine taking a measurement of the spin along the x axis. Using the formulas above, we can discover \\(\\langle \\psi_+ \\mid S_x \\mid \\psi_+ \\rangle = 1\\) and \\(\\langle \\psi_- \\mid S_x \\mid \\psi_- \\rangle = -1\\) , so that the total average measured spin will be 0. What if we used a density matrix? Then we would find out: \\[ \\rho_+ = \\mid \\psi_+ \\rangle \\langle \\psi_+ \\mid = \\begin{bmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2} \\end{bmatrix} \\qquad \\rho_- = \\mid \\psi_- \\rangle \\langle \\psi_- \\mid = \\begin{bmatrix} \\frac{1}{2} & -\\frac{1}{2} \\\\ -\\frac{1}{2} & \\frac{1}{2} \\end{bmatrix}. \\] Because equations involving the density matrix are linear, we can carry out the average immediately and finding a collective density matrix describing the whole ensemble (something that is not possible with the individual wavefunctions): \\[ \\rho_{tot} = \\frac{\\rho_++\\rho_-}{2} = \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix}. \\] This density matrix describes a mixed state, as it can not be expressed as the outer product of any vector with itself. We can then compute the expectation value of the operator \\(S_x\\) : \\[ \\langle S_x \\rangle = \\mathrm{Tr}(\\rho_{tot} S_x) = 0. \\] This is a very simple example of decoherence - because we are measuring a statistical ensemble of quantum systems, rather than a single spin, some information on the phase factors of its wavefunctions (the off-diagonal terms of the density matrix) can be averaged out and lost. In real life we almost never observe spins in isolations, and spin ensembles that have had a long time to exchange energy with their environment and all its random thermal fluctuations are highly decohered, as each individual spin has had its own dynamical history. This is effectively the case for the spins one usually finds inside a sample when performing a muon spin resonance experiment. For this reason we need to use density matrices when dealing with systems that have been initialised in a thermal state, as well as when trying to approximate the interaction of an open quantum system with its surrounding environment, exchanging energy with it and thus relaxing towards a thermal state. For developers: the DensityOperator class inherits from Operator and is found in muspinsim/spinop.py . Systems of multiple spins Combining spin states Systems of only one spin are not very interesting for our purposes. A lot of muon spin resonance experiments involve some kind of interaction - either hyperfine interaction between a muon and an electron in a radical, or hyperfine mediated interaction between the muon and another atomic nucleus (like hydrogen), or dipolar interaction, and so on. To see how we build such a compound state, let's consider the case of an electron and a muon, two spin-\u00bd particles. If each particle is described on a basis of \\(\\mid \\uparrow \\rangle\\) and \\(\\mid \\downarrow \\rangle\\) , then the combined system has four possible states, corresponding to all permutations of individual states: \\(\\mid \\uparrow \\uparrow \\rangle\\) , \\(\\mid \\uparrow \\downarrow \\rangle\\) , \\(\\mid \\downarrow \\uparrow \\rangle\\) and \\(\\mid \\downarrow \\downarrow \\rangle\\) . In general, a system of \\(N\\) spin-\u00bd particles will have \\(2^N\\) possible states. If the two spins were prepared independently in their own state, the state of the combined system can be built using the so-called Kronecker product between vectors: \\[ \\mid\\psi_{\\mu}\\rangle = \\begin{bmatrix} \\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\end{bmatrix} \\qquad \\mid\\psi_{e}\\rangle = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\qquad \\mid\\psi_{\\mu,e}\\rangle = \\mid\\psi_{\\mu}\\rangle \\otimes \\mid\\psi_{e}\\rangle = \\begin{bmatrix} \\frac{\\sqrt{2}}{2}\\cdot \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\\\ \\frac{\\sqrt{2}}{2}\\cdot \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\sqrt{2}}{2} \\\\ 0 \\\\ \\frac{\\sqrt{2}}{2} \\\\ 0 \\end{bmatrix} \\] In this convention, we say the index of the electron states updates faster (moving down the column vector we change electron state more rapidly than we do muon states). Of course, it would be possible to also decide for the opposite convention and have the index of the muon states be the faster ones - it does not matter as long as we keep our convention consistent throughout all the calculations that follow. The same exact approach can be used for density matrices too. Consider the following state, describing a muon polarised along x and an unpolarised electron. This is a typical starting state to use for muon spin dynamics simulations: \\[ \\rho_\\mu = \\begin{bmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2} \\end{bmatrix} \\qquad \\rho_e = \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix} \\] \\[ \\rho_{\\mu,e} = \\rho_{\\mu}\\otimes \\rho_{e} = \\begin{bmatrix} \\frac{1}{2}\\cdot \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix} & \\frac{1}{2}\\cdot \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix} \\\\ \\frac{1}{2}\\cdot \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix} & \\frac{1}{2}\\cdot \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{4} & 0 & \\frac{1}{4} & 0 \\\\ 0 & \\frac{1}{4} & 0 & \\frac{1}{4} \\\\ \\frac{1}{4} & 0 & \\frac{1}{4} & 0 \\\\ 0 & \\frac{1}{4} & 0 & \\frac{1}{4} \\end{bmatrix} \\] Any state built by Kronecker product of two individual states will have the particles acting effectively independently from one another. This is not always the case. Evolution under an Hamiltonian that couples two spins will produce correlations between them. Consider the following vector state: \\[ \\mid \\psi_{corr} \\rangle = \\begin{bmatrix} \\frac{\\sqrt{2}}{2} \\\\ 0 \\\\ 0 \\\\ \\frac{\\sqrt{2}}{2} \\end{bmatrix} \\] In this state, the values measured on one particle depend on those measured on the other - either they're both up, or they're both down. This is an example of a state that can not be obtained by simply multiplying together two single particle states, because it's an example of entanglement . For developers: all classes inheriting from Operator have a .kron() method that allows Kronecker products with other operators, which will internally keep track of the dimensions of the system in order to check for compatibility in any subsequent operations. Combining operators We've seen in the previous section how to combine multiple density matrices. Since density matrices are operators, it should be clear that the rules for combining operators are exactly the same, using the Kronecker product of individual matrices. For example, a term \\(S_z^\\mu S_x^e\\) in matrix form will be: \\[ S_z^\\mu S_x^e = \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix} \\otimes \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 &-1 \\\\ 0 & 0 &-1 & 0 \\end{bmatrix} \\] Sometimes, even in a system with multiple spins, operators involving only one of them, like \\(S_x^\\mu\\) , might be relevant. In that case we must understand them as having an implicit identity matrix for all the spins that don't appear explicitly. So we have: \\[ S_x^\\mu = S_x^\\mu\\mathbb{I}^e = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\otimes \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 &1 \\\\ 1 & 0 & 0 & 0 \\\\ 0 &1 & 0 & 0 \\\\ \\end{bmatrix} \\] Spin-spin couplings All spin resonance experiments are driven by interactions between the muon and other spins. These interactions can have different nature, but are generally described by tensors. In MuSpinSim, one can either input these tensors in (as calculated with an ab initio software, for example), or run a fitting routine to try and infer them from the results of an experiment. These tensors describe an interaction in space , and thus are always \\(3\\times 3\\) matrices. How does one build these coupling terms with the formalism we're using? Let's consider a simple case: an electron and a muon coupled by a hyperfine tensor \\(\\mathbf{A}\\) in zero external magnetic field. The Hamiltonian is then calculated as follows: \\[ \\begin{split} \\mathcal{H} = & \\mathbf{S}^\\mu\\mathbf{A}\\mathbf{S}^e = \\begin{bmatrix} S^\\mu_x & S^\\mu_y & S^\\mu_z \\end{bmatrix} \\begin{bmatrix} A_{xx} & A_{xy} & A_{xz} \\\\ A_{xy} & A_{yy} & A_{yz} \\\\ A_{xz} & A_{yz} & A_{zz} \\end{bmatrix} \\begin{bmatrix} S^e_x \\\\ S^e_y \\\\ S^e_z \\end{bmatrix} = \\\\ = & A_{xx}S^\\mu_xS^e_x + A_{yy}S^\\mu_yS^e_y + A_{zz}S^\\mu_zS^e_z + \\\\ +&A_{xy}(S^\\mu_xS^e_y+S^\\mu_yS^e_x) + A_{xz}(S^\\mu_xS^e_z+S^\\mu_zS^e_x) + A_{yz}(S^\\mu_yS^e_z+S^\\mu_zS^e_y) \\end{split} \\] In other words, it is a sum of operators as the ones described above. If the spin system included more spins than just the muon and electron, those would have to be implicitly included too as identity matrices in the Kronecker products. In the case of a two spin system, the final sum can be seen as a sum of \\(4 \\times 4\\) matrices, whereas the vectors \\(\\mathbf{S}^\\mu\\) and \\(\\mathbf{S}^e\\) are really \"vectors of matrices\". Numerically, we would store them as \\(3\\times4\\times4\\) arrays. For developers: tensor products involving spin operators and the creation of terms like the above are handled by the InteractionTerm class and its children, found in muspinsim/spinsys.py . Next up, we'll look at how calculations are actually initialised and run in MuSpinSim .","title":"Theory of spin dynamics - I"},{"location":"theory_1/#theory-of-spin-dynamics-i","text":"","title":"Theory of spin dynamics - I"},{"location":"theory_1/#introduction","text":"Spin is an essentially quantum mechanical phenomenon. While single spins can sometimes be usefully visualised in a classical way, as dipoles with a definite direction that are subject to rotation by precession under applied fields, this classical description quickly breaks down when multiple coupled spins are involved - and that is the premise of any and all mildly interesting muon science experiments. So, there's very little way out of the fact that muon experiments have to be described with quantum mechanical equations. Here we're going to give a quick overview of the relevant equations, their meaning, and the matrix formalism that is used to implement them numerically in MuSpinSim.","title":"Introduction"},{"location":"theory_1/#the-basics-spin-states-and-hamiltonians","text":"","title":"The basics: spin states and Hamiltonians"},{"location":"theory_1/#spin-states-as-vectors","text":"Quantum mechanics is often taught first with an eye to particles, like electrons, which are described by a complex-valued wavefunction \\(\\psi(x)\\) all across the three-dimensional space. The value of the wave function at a point corresponds to the amplitude of finding the particle at that point. Quantum amplitudes act like probabilities, in the sense that their square modulus \\(\\psi^*\\psi\\) expresses the probability of finding the particle at that specific point, but being complex numbers have the peculiar property that they can interfere with themselves constructively or destructively, leading to many of quantum mechanics' most counter-intuitive results (such as the way an electron's probability amplitudes add up to fringes in the double slit experiment). When it comes to individual spins, the situation is not that different, except for the fact that the wavefunction is not defined over an infinite amount of points in \\(\\mathbb{R}^3\\) ; instead, it is defined over a discrete number of states that the spin can occupy. Specifically, for a particle with spin \\(S\\) , there are \\(2S+1\\) possible states. In the simplest case, a spin-\u00bd particle, there are just two possible states: up, \\(\\mid\\uparrow\\rangle\\) and down, \\(\\mid\\downarrow\\rangle\\) . For this reason, a spin-\u00bd particle can also be considered a \"quantum bit\", or qubit . Electrons, muons and protons (namely, hydrogen nuclei) are all spin-\u00bd particles. The wavefunction of a spin-\u00bd particle can therefore be expressed with just two complex coefficients: \\[ \\mid\\psi_{1/2}\\rangle = a\\mid\\uparrow\\rangle + b\\mid\\downarrow\\rangle \\] with \\(a^*a+b^*b = 1\\) as a normalisation condition. One possible convention to write this wavefunction and manipulate it in a computer program is to treat these states as the basis for a vector space (which effectively, they are: they obey the same inner product rules as the versors for a regular Euclidean 2D space). Then we can write the wavefunction as a column vector: \\[ \\mid\\uparrow\\rangle = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\qquad \\mid\\downarrow\\rangle = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\qquad \\mid\\psi_{1/2}\\rangle = \\begin{bmatrix} a \\\\ b \\end{bmatrix} \\] Conversely, we can write the complex conjugate versions of these states (the \"bras\" to these \"kets\") as row vectors: \\[ \\langle\\uparrow\\mid = \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\qquad \\langle\\downarrow\\mid = \\begin{bmatrix} 0 & 1 \\end{bmatrix} \\qquad \\langle\\psi_{1/2}\\mid = \\begin{bmatrix} a^* & b^* \\end{bmatrix} \\] (where of course it's important to remember that the coefficients too are to be conjugated). That way, one can see for example how the inner product results naturally as a scalar product between vectors.","title":"Spin states as vectors"},{"location":"theory_1/#operators-as-matrices","text":"If we are describing spin states as vectors, operators can be described as matrices instead. In particular, for the spin-\u00bd particle, we have the Pauli matrices: \\[ S_x = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\qquad S_y = \\begin{bmatrix} 0 & -i \\\\ i & 0 \\end{bmatrix} \\qquad S_z = \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix} \\] for the three spatial components of the spin. Together with the identity matrix, these form a complete basis for the operators for this kind of spin. These operators can be easily used to compute expectation values. For example, consider the following state: \\[ \\psi = \\begin{bmatrix} \\frac{\\sqrt{3}}{2} \\\\ \\frac{1}{2}i \\end{bmatrix} \\] We can find the expectation values of its components by applying simple matrix product rules: \\[ \\langle S_x \\rangle = \\langle\\psi\\mid S_z \\mid\\psi\\rangle = \\begin{bmatrix} \\frac{\\sqrt{3}}{2} & -\\frac{1}{2}i \\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\begin{bmatrix} \\frac{\\sqrt{3}}{2} \\\\ \\frac{1}{2}i \\end{bmatrix} = \\begin{bmatrix} \\frac{\\sqrt{3}}{2} & -\\frac{1}{2}i \\end{bmatrix} \\begin{bmatrix} \\frac{1}{2}i \\\\ \\frac{\\sqrt{3}}{2} \\end{bmatrix} = 0 \\] \\[ \\langle S_y \\rangle = \\langle\\psi\\mid S_z \\mid\\psi\\rangle = \\begin{bmatrix} \\frac{\\sqrt{3}}{2} & -\\frac{1}{2}i \\end{bmatrix} \\begin{bmatrix} 0 & -i \\\\ i & 0 \\end{bmatrix} \\begin{bmatrix} \\frac{\\sqrt{3}}{2} \\\\ \\frac{1}{2}i \\end{bmatrix} = \\begin{bmatrix} \\frac{\\sqrt{3}}{2} & -\\frac{1}{2}i \\end{bmatrix} \\begin{bmatrix} \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2}i \\end{bmatrix} = \\frac{\\sqrt{3}}{2} \\] \\[ \\langle S_z \\rangle = \\langle\\psi\\mid S_z \\mid\\psi\\rangle = \\begin{bmatrix} \\frac{\\sqrt{3}}{2} & -\\frac{1}{2}i \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix} \\begin{bmatrix} \\frac{\\sqrt{3}}{2} \\\\ \\frac{1}{2}i \\end{bmatrix} = \\begin{bmatrix} \\frac{\\sqrt{3}}{2} & -\\frac{1}{2}i \\end{bmatrix} \\begin{bmatrix} \\frac{\\sqrt{3}}{2} \\\\ -\\frac{1}{2}i \\end{bmatrix} = \\frac{1}{2} \\] We can learn a few things from it. The three expectation values along x, y, z would correspond, classically, to the three components of the spin's magnetic moment. In classical terms, this would be a vector making a \\(60\u00b0\\) angle with the vertical, pointing towards the y direction. In general this example shows two important features of spin-\u00bd states: the component along z of the moment is determined by the relative probabilities of finding the spin up or down. If the probability are equal, the component along z is zero; the component in the xy plane can only be non-zero if the spin exists in some mixture of up and down states, and it's maximum if the up and down states have amplitudes with the same modulus. The specific direction of the component is then controlled by the relative phase of those amplitudes. For developers : quantum operators are defined by the class Operator and derived classes in muspinsim/spinop.py .","title":"Operators as matrices"},{"location":"theory_1/#hamiltonian-and-time-evolution","text":"Hamiltonians are operators with the additional required property of being Hermitian , that is, they have to be identical to their conjugate transpose. For a spin system, the Hamiltonian is the operator whose expectation value is the energy of that system. The Hamiltonian also controls the time evolution of the system; like for every other quantum system, the dynamics of a spin system are defined by the equation \\[ H\\mid\\psi\\rangle = i\\hbar\\frac{\\partial}{\\partial t}\\mid\\psi\\rangle, \\] whose solution is \\[ \\mid \\psi(t) \\rangle = e^{-\\frac{i}{\\hbar}Ht} \\mid \\psi(0) \\rangle. \\] The key task of MuSpinSim is to solve this equation in time, and then estimate the expectation values of the observables that interest us. The simplest way to do so in the case of a small system is to diagonalise the Hamiltonian, which numerically can be done fairly easily by taking advantage of its properties (for example using the NumPy routine numpy.linalg.eigh ). This gives a number of eigenvalues \\(\\lambda_i\\) and corresponding eigenvectors (namely, eigenstates) \\(\\mid u_i \\rangle\\) . One can then write the Hamiltonian matrix as: $$ H = \\sum_i \\lambda_i \\mid u_i \\rangle \\langle u_i \\mid = UH_0U^\\dagger $$ where \\(H_0\\) is a diagonal matrix with the eigenvalues along its diagonal, and \\(U\\) is the matrix with the eigenvectors for columns (and \\(U^\\dagger\\) its conjugate transpose). One can then transform the wavefunction in this new basis, and the matrix exponential of the now diagonal Hamiltonian becomes trivial. For developers: the Hamiltonian class is a mixin inheriting from Operator and Hermitian and is found in muspinsim/hamiltonian.py .","title":"Hamiltonian and time evolution"},{"location":"theory_1/#the-density-matrix-formalism","text":"Until here we've focused on wave functions as a way to write quantum states. However, in practice, in MuSpinSim we never use simple wave functions to express the state of a system - rather, we use density matrices . The density matrix formalism is a generalisation of the state vectors we described above that allows us to describe statistical ensembles of quantum states, rather than just individual pure states. Density matrices are especially useful and important when dealing with spin systems at a thermal equilibrium, which is what makes them so essential in MuSpinSim, as any spin other than the muon itself is usually in a thermal state at the beginning of the experiment. The density matrix is an operator whose expectaction value with a certain state is the probability to find the system in that state. For a pure state in a spin-\u00bd system as the one described above, the density matrix would be \\[ \\rho = a^*a \\mid \\uparrow \\rangle \\langle \\uparrow \\mid + a^*b \\mid \\downarrow \\rangle \\langle \\uparrow \\mid + b^*a \\mid \\uparrow \\rangle \\langle \\downarrow \\mid + b^*b \\mid \\downarrow \\rangle \\langle \\downarrow \\mid = \\begin{bmatrix} a \\\\ b \\end{bmatrix} \\begin{bmatrix} a^* & b^* \\end{bmatrix} = \\begin{bmatrix} a^*a & b^*a \\\\ a^*b & b^*b \\end{bmatrix}. \\] This type of product between vectors is called the outer product . One can see how the normalisation rule implies that \\(\\mathrm{Tr}(\\rho) = 1\\) . The expectation value of an operator can be found \\[ \\langle O \\rangle = \\mathrm{Tr}(\\rho O) \\] and the time evolution is \\[ \\frac{\\partial \\rho}{\\partial t} = -\\frac{i}{\\hbar}[H, \\rho] \\qquad\\implies\\qquad \\rho(t) = e^{-\\frac{i}{\\hbar}Ht}\\rho(0)e^{\\frac{i}{\\hbar}Ht}. \\] To see an example of the usefulness of density matrices, let's consider again a spin-\u00bd example. Consider an ensemble of particles prepared such that half of them is prepared in a state \\(\\psi_+\\) and the other half in a state \\(\\psi_-\\) : \\[ \\mid\\psi_+\\rangle = \\begin{bmatrix} \\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\end{bmatrix} \\qquad \\mid\\psi_-\\rangle = \\begin{bmatrix} \\frac{\\sqrt{2}}{2} \\\\ \\frac{-\\sqrt{2}}{2} \\end{bmatrix}. \\] Now imagine taking a measurement of the spin along the x axis. Using the formulas above, we can discover \\(\\langle \\psi_+ \\mid S_x \\mid \\psi_+ \\rangle = 1\\) and \\(\\langle \\psi_- \\mid S_x \\mid \\psi_- \\rangle = -1\\) , so that the total average measured spin will be 0. What if we used a density matrix? Then we would find out: \\[ \\rho_+ = \\mid \\psi_+ \\rangle \\langle \\psi_+ \\mid = \\begin{bmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2} \\end{bmatrix} \\qquad \\rho_- = \\mid \\psi_- \\rangle \\langle \\psi_- \\mid = \\begin{bmatrix} \\frac{1}{2} & -\\frac{1}{2} \\\\ -\\frac{1}{2} & \\frac{1}{2} \\end{bmatrix}. \\] Because equations involving the density matrix are linear, we can carry out the average immediately and finding a collective density matrix describing the whole ensemble (something that is not possible with the individual wavefunctions): \\[ \\rho_{tot} = \\frac{\\rho_++\\rho_-}{2} = \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix}. \\] This density matrix describes a mixed state, as it can not be expressed as the outer product of any vector with itself. We can then compute the expectation value of the operator \\(S_x\\) : \\[ \\langle S_x \\rangle = \\mathrm{Tr}(\\rho_{tot} S_x) = 0. \\] This is a very simple example of decoherence - because we are measuring a statistical ensemble of quantum systems, rather than a single spin, some information on the phase factors of its wavefunctions (the off-diagonal terms of the density matrix) can be averaged out and lost. In real life we almost never observe spins in isolations, and spin ensembles that have had a long time to exchange energy with their environment and all its random thermal fluctuations are highly decohered, as each individual spin has had its own dynamical history. This is effectively the case for the spins one usually finds inside a sample when performing a muon spin resonance experiment. For this reason we need to use density matrices when dealing with systems that have been initialised in a thermal state, as well as when trying to approximate the interaction of an open quantum system with its surrounding environment, exchanging energy with it and thus relaxing towards a thermal state. For developers: the DensityOperator class inherits from Operator and is found in muspinsim/spinop.py .","title":"The density matrix formalism"},{"location":"theory_1/#systems-of-multiple-spins","text":"","title":"Systems of multiple spins"},{"location":"theory_1/#combining-spin-states","text":"Systems of only one spin are not very interesting for our purposes. A lot of muon spin resonance experiments involve some kind of interaction - either hyperfine interaction between a muon and an electron in a radical, or hyperfine mediated interaction between the muon and another atomic nucleus (like hydrogen), or dipolar interaction, and so on. To see how we build such a compound state, let's consider the case of an electron and a muon, two spin-\u00bd particles. If each particle is described on a basis of \\(\\mid \\uparrow \\rangle\\) and \\(\\mid \\downarrow \\rangle\\) , then the combined system has four possible states, corresponding to all permutations of individual states: \\(\\mid \\uparrow \\uparrow \\rangle\\) , \\(\\mid \\uparrow \\downarrow \\rangle\\) , \\(\\mid \\downarrow \\uparrow \\rangle\\) and \\(\\mid \\downarrow \\downarrow \\rangle\\) . In general, a system of \\(N\\) spin-\u00bd particles will have \\(2^N\\) possible states. If the two spins were prepared independently in their own state, the state of the combined system can be built using the so-called Kronecker product between vectors: \\[ \\mid\\psi_{\\mu}\\rangle = \\begin{bmatrix} \\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\end{bmatrix} \\qquad \\mid\\psi_{e}\\rangle = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\qquad \\mid\\psi_{\\mu,e}\\rangle = \\mid\\psi_{\\mu}\\rangle \\otimes \\mid\\psi_{e}\\rangle = \\begin{bmatrix} \\frac{\\sqrt{2}}{2}\\cdot \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\\\ \\frac{\\sqrt{2}}{2}\\cdot \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\sqrt{2}}{2} \\\\ 0 \\\\ \\frac{\\sqrt{2}}{2} \\\\ 0 \\end{bmatrix} \\] In this convention, we say the index of the electron states updates faster (moving down the column vector we change electron state more rapidly than we do muon states). Of course, it would be possible to also decide for the opposite convention and have the index of the muon states be the faster ones - it does not matter as long as we keep our convention consistent throughout all the calculations that follow. The same exact approach can be used for density matrices too. Consider the following state, describing a muon polarised along x and an unpolarised electron. This is a typical starting state to use for muon spin dynamics simulations: \\[ \\rho_\\mu = \\begin{bmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2} \\end{bmatrix} \\qquad \\rho_e = \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix} \\] \\[ \\rho_{\\mu,e} = \\rho_{\\mu}\\otimes \\rho_{e} = \\begin{bmatrix} \\frac{1}{2}\\cdot \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix} & \\frac{1}{2}\\cdot \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix} \\\\ \\frac{1}{2}\\cdot \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix} & \\frac{1}{2}\\cdot \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{4} & 0 & \\frac{1}{4} & 0 \\\\ 0 & \\frac{1}{4} & 0 & \\frac{1}{4} \\\\ \\frac{1}{4} & 0 & \\frac{1}{4} & 0 \\\\ 0 & \\frac{1}{4} & 0 & \\frac{1}{4} \\end{bmatrix} \\] Any state built by Kronecker product of two individual states will have the particles acting effectively independently from one another. This is not always the case. Evolution under an Hamiltonian that couples two spins will produce correlations between them. Consider the following vector state: \\[ \\mid \\psi_{corr} \\rangle = \\begin{bmatrix} \\frac{\\sqrt{2}}{2} \\\\ 0 \\\\ 0 \\\\ \\frac{\\sqrt{2}}{2} \\end{bmatrix} \\] In this state, the values measured on one particle depend on those measured on the other - either they're both up, or they're both down. This is an example of a state that can not be obtained by simply multiplying together two single particle states, because it's an example of entanglement . For developers: all classes inheriting from Operator have a .kron() method that allows Kronecker products with other operators, which will internally keep track of the dimensions of the system in order to check for compatibility in any subsequent operations.","title":"Combining spin states"},{"location":"theory_1/#combining-operators","text":"We've seen in the previous section how to combine multiple density matrices. Since density matrices are operators, it should be clear that the rules for combining operators are exactly the same, using the Kronecker product of individual matrices. For example, a term \\(S_z^\\mu S_x^e\\) in matrix form will be: \\[ S_z^\\mu S_x^e = \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix} \\otimes \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 &-1 \\\\ 0 & 0 &-1 & 0 \\end{bmatrix} \\] Sometimes, even in a system with multiple spins, operators involving only one of them, like \\(S_x^\\mu\\) , might be relevant. In that case we must understand them as having an implicit identity matrix for all the spins that don't appear explicitly. So we have: \\[ S_x^\\mu = S_x^\\mu\\mathbb{I}^e = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\otimes \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 &1 \\\\ 1 & 0 & 0 & 0 \\\\ 0 &1 & 0 & 0 \\\\ \\end{bmatrix} \\]","title":"Combining operators"},{"location":"theory_1/#spin-spin-couplings","text":"All spin resonance experiments are driven by interactions between the muon and other spins. These interactions can have different nature, but are generally described by tensors. In MuSpinSim, one can either input these tensors in (as calculated with an ab initio software, for example), or run a fitting routine to try and infer them from the results of an experiment. These tensors describe an interaction in space , and thus are always \\(3\\times 3\\) matrices. How does one build these coupling terms with the formalism we're using? Let's consider a simple case: an electron and a muon coupled by a hyperfine tensor \\(\\mathbf{A}\\) in zero external magnetic field. The Hamiltonian is then calculated as follows: \\[ \\begin{split} \\mathcal{H} = & \\mathbf{S}^\\mu\\mathbf{A}\\mathbf{S}^e = \\begin{bmatrix} S^\\mu_x & S^\\mu_y & S^\\mu_z \\end{bmatrix} \\begin{bmatrix} A_{xx} & A_{xy} & A_{xz} \\\\ A_{xy} & A_{yy} & A_{yz} \\\\ A_{xz} & A_{yz} & A_{zz} \\end{bmatrix} \\begin{bmatrix} S^e_x \\\\ S^e_y \\\\ S^e_z \\end{bmatrix} = \\\\ = & A_{xx}S^\\mu_xS^e_x + A_{yy}S^\\mu_yS^e_y + A_{zz}S^\\mu_zS^e_z + \\\\ +&A_{xy}(S^\\mu_xS^e_y+S^\\mu_yS^e_x) + A_{xz}(S^\\mu_xS^e_z+S^\\mu_zS^e_x) + A_{yz}(S^\\mu_yS^e_z+S^\\mu_zS^e_y) \\end{split} \\] In other words, it is a sum of operators as the ones described above. If the spin system included more spins than just the muon and electron, those would have to be implicitly included too as identity matrices in the Kronecker products. In the case of a two spin system, the final sum can be seen as a sum of \\(4 \\times 4\\) matrices, whereas the vectors \\(\\mathbf{S}^\\mu\\) and \\(\\mathbf{S}^e\\) are really \"vectors of matrices\". Numerically, we would store them as \\(3\\times4\\times4\\) arrays. For developers: tensor products involving spin operators and the creation of terms like the above are handled by the InteractionTerm class and its children, found in muspinsim/spinsys.py . Next up, we'll look at how calculations are actually initialised and run in MuSpinSim .","title":"Spin-spin couplings"},{"location":"theory_2/","text":"Theory of spin dynamics - II Preparing the initial state When performing a simulation of a muon experiment, the first step is to prepare the system in an appropriate quantum state to evolve under the Hamiltonian. MuSpinSim uses the following rules to prepare this state: the muon is prepared in a state polarised along the direction of the beam (conventionally, the x axis in the laboratory frame of reference); every other spin is prepared in a thermal density matrix state. A thermal density matrix state simply means a state in which every energy level is populated with a probability following the Boltzmann distribution, and fully decohered otherwise. In other words: \\[ \\rho_{th}(T) = \\frac{e^{-\\frac{\\mathcal{H}}{k_BT}}}{\\mathrm{Tr}\\left(e^{-\\frac{\\mathcal{H}}{k_BT}}\\right)} \\] where the trace below is the partition function of the system. One can see how finding this matrix would in principle require diagonalising the Hamiltonian of the whole system the muon is being inserted in. In practice, in MuSpinSim we use one of two approximations: by default, the \\(T=\\infty\\) approximation is used, in which all states are equally populated. The advantage of this approximation is that it's completely invariant to any change of basis - it doesn't make a difference what exactly the eigenstates of the Hamiltonian are. The real temperature of the sample, of course, is not infinite, but as long as \\(k_BT \\gg \\mathcal{H}\\) , that's a fair enough approximation; if requested by the user, finite temperature can be used, but the Hamiltonian is simplified to the Zeeman Hamiltonian, \\(\\mathcal{H} \\approx \\hbar \\sum_i\\gamma_i \\mathbf{B}\\mathbf{S}^{(i)}\\) . This approximation keeps all spins independent from each other, ignoring their possible couplings, and is very effective in the case in which a strong magnetic field is applied and the Zeeman term is prevalent in the Hamiltonian. It should not be used for zero field experiment simulations. For developers: the initial density matrix is calculated in the property .rho0() of the class ExperimentRunner , found in muspinsim/experiment.py . Time evolution Closed system Time evolution for the density matrix of a closed quantum system is controlled by the Liouville-von Neumann equation that we've already seen: \\[ \\frac{\\partial \\rho}{\\partial t} = -\\frac{i}{\\hbar}[\\mathcal{H}, \\rho] \\] If we write the matrix quantities with indices (repeated indices imply summation) we can write this as a system of coupled differential equations for each individual coefficient: \\[ \\frac{\\partial \\rho_{ij}}{\\partial t} = -\\frac{i}{\\hbar}\\left(\\mathcal{H}_{ik}\\rho_{kj}-\\rho_{ik}\\mathcal{H}_{kj}\\right) \\] This gets significantly simpler when we express both the matrices in a basis in which the Hamiltonian is diagonal, and thus \\(\\mathcal{H}_{ij} = \\lambda_i\\delta_{ij}\\) : \\[ \\frac{\\partial \\rho_{ij}}{\\partial t} = -\\frac{i}{\\hbar}\\rho_{ij}\\left(\\lambda_i-\\lambda_j\\right) \\qquad \\implies \\qquad \\rho_{ij}(t) = e^{-\\frac{i}{\\hbar}\\left(\\lambda_i-\\lambda_j\\right) t}\\rho_{ij}(0) \\] This way we can see that the equations are completely decoupled. Coefficients on the diagonal of the density matrix don't change, while off-diagonal coefficients gain a phase factor at a constant rate that is dependent on the differences between the Hamiltonian eigenvalues. This method gives us the exact evolution of the system and perfectly preserves unitarity. The downside of it is that it requires a full diagonalization of the Hamiltonian. However, many spin systems that we are interested in are relatively small, and one single diagonalisation for each of them isn't a big deal. Cheaper, more approximate methods might be implemented in the future, but at the moment, MuSpinSim can easily carry out calculations on systems that don't exceed nine or ten spins on a laptop in a few minutes. For developers: time evolution of a system is handled by the .evolve() method of the Hamiltonian class. Integral of asymmetry In muon experiments we're usually interested in measuring the asymmetry of positron hits between the forward and back detectors in the experimental setup - namely, the polarisation of the muon along a certain axis, as it evolves in time. However, in some cases (like ALC experiments) what we actually care about is the integral of this asymmetry throughout a certain time interval. This could be trivially computed simply by computing the time evolution and then integrating numerically. However MuSpinSim in this case uses a different algorithm to perform the integral analytically, saving some unnecessary steps. The full derivation of the formula is detailed in this arXiv paper . The essence of it is that, if we have an operator \\(S\\) with matrix elements \\(s_{ij}\\) whose integral value we want to compute: $$ \\langle P \\rangle = \\int_0^\\infty \\langle S \\rangle(t) e^{-\\frac{t}{\\tau}} dt $$ where the integral is weighed with the decay process of the muon with lifetime \\(\\tau\\) , then we can define a new operator \\(P\\) with matrix elements: \\[ p_{ij} = \\frac{s_{ij}}{\\frac{1}{\\tau}-\\frac{i}{\\hbar}\\left(\\lambda_i-\\lambda_j\\right)} \\] and evaluating its expectation value on the initial state of the system will in a single pass return the value of the desired integral. For developers: integral expectation values are handled by the .integrate_decaying() method of the Hamiltonian class. Open system Systems described by the Liouville-von Neumann equation are closed; they conserve energy and evolve in a perfectly reversible way. This is sometimes not a good approximation, because in real life, the chunk of the sample that we're describing is of course only a small part of a much bigger system, fully coupled to it and interacting in a lot of ways. Since including an environment of hundreds or thousands of spins is not practical, a more common approach is to use a master equation that allows to describe irreversible evolution through some kind of energy exchange with environmental degrees of freedom. In MuSpinSim, the only such master equation that is supported is the simplest one, the Lindblad equation. It is an extension of the Liouville-von Neumann equation including dissipative terms: \\[ \\frac{\\partial \\rho}{\\partial t} = -\\frac{i}{\\hbar}[\\mathcal{H}, \\rho] + \\sum_{i=1}^{N^2-1}\\alpha_i\\left(L_i \\rho L_i^\\dagger - \\frac{1}{2}\\left\\{L_i^\\dagger L_i, \\rho\\right\\} \\right) \\] Here the \\(\\alpha_i\\) are coefficients that express the strength of the coupling with a certain degree of freedom, and the \\(L_i\\) are the so-called Lindblad or jump operators of the system, each connected to one coefficient. The curly braces denote the anticommutator of two matrices: \\(\\{A, B\\} = AB+BA\\) . This equation unfortunately does not have a neat solution in exponential form as the one seen above in the matrix formalism. It is however possible to find something very close to it by making a few small changes in the representation, namely, expressing the density matrix in what is called the Fock-Liouville space . An excellent and detailed explanation of this technique is given in this useful introductory paper by Daniel Manzano . The essence of it is that we \"straighten up\" the density matrix, writing all its elements in a single column vector. For example, a \\(4\\times 4\\) matrix can turn into a \\(16\\) elements column vector. It is then possible to write a matrix called the Lindbladian (that in the example will be \\(16 \\times 16\\) ) that operates on it exactly like a Hamiltonian does on a single wavefunction: \\[ \\frac{\\partial}{\\partial t} \\mid \\rho \\rangle\\rangle = \\mathcal{L} \\mid \\rho \\rangle\\rangle \\] and following from that, it is possible to integrate the equations as trivially as seen for the others by diagonalising the Lindbladian. Care must be taken though because unlike for the Hamiltonian, there is no guarantee that the Lindbladian is Hermitian, or for that matter, diagonalizable at all! This can potentially cause issues - however in my experience well-defined systems will be solvable without problems. In MuSpinSim, the only way dissipation can be included in a calculation is by putting an individual spin in contact with a thermal reserve. This is done by defining two jump operators for that spin, \\(S_+^i\\) and \\(S_-^i\\) , and the corresponding dissipation coefficients such that \\[ \\frac{\\alpha_+^i}{\\alpha_-^i} = \\exp\\left(-\\frac{\\hbar\\gamma |B|}{k_BT}\\right) \\] where \\(T\\) is the temperature of the system, and \\(\\hbar\\gamma|B|\\) is an approximation using only the Zeeman interaction of the energy gap between successive states of the spin. For \\(T < \\infty\\) , this is subject to the same limits as the choice of using only the Zeeman interaction to define the initial thermal state density matrix. In fact, the effect of these terms is to tend to drive the individual spin's state towards exactly that thermal state, adding or removing energy as needed and erasing coherences. For developers: the Lindbladian class is defined in muspinsim/lindbladian.py . It has .evolve() and .integrate_decaying() methods analogous to those of the Hamiltonian class. In the next section we will look specifically at the exact shape of the terms of the Hamiltonian (and when necessary, Lindbladian) used in MuSpinSim.","title":"Theory of spin dynamics - II"},{"location":"theory_2/#theory-of-spin-dynamics-ii","text":"","title":"Theory of spin dynamics - II"},{"location":"theory_2/#preparing-the-initial-state","text":"When performing a simulation of a muon experiment, the first step is to prepare the system in an appropriate quantum state to evolve under the Hamiltonian. MuSpinSim uses the following rules to prepare this state: the muon is prepared in a state polarised along the direction of the beam (conventionally, the x axis in the laboratory frame of reference); every other spin is prepared in a thermal density matrix state. A thermal density matrix state simply means a state in which every energy level is populated with a probability following the Boltzmann distribution, and fully decohered otherwise. In other words: \\[ \\rho_{th}(T) = \\frac{e^{-\\frac{\\mathcal{H}}{k_BT}}}{\\mathrm{Tr}\\left(e^{-\\frac{\\mathcal{H}}{k_BT}}\\right)} \\] where the trace below is the partition function of the system. One can see how finding this matrix would in principle require diagonalising the Hamiltonian of the whole system the muon is being inserted in. In practice, in MuSpinSim we use one of two approximations: by default, the \\(T=\\infty\\) approximation is used, in which all states are equally populated. The advantage of this approximation is that it's completely invariant to any change of basis - it doesn't make a difference what exactly the eigenstates of the Hamiltonian are. The real temperature of the sample, of course, is not infinite, but as long as \\(k_BT \\gg \\mathcal{H}\\) , that's a fair enough approximation; if requested by the user, finite temperature can be used, but the Hamiltonian is simplified to the Zeeman Hamiltonian, \\(\\mathcal{H} \\approx \\hbar \\sum_i\\gamma_i \\mathbf{B}\\mathbf{S}^{(i)}\\) . This approximation keeps all spins independent from each other, ignoring their possible couplings, and is very effective in the case in which a strong magnetic field is applied and the Zeeman term is prevalent in the Hamiltonian. It should not be used for zero field experiment simulations. For developers: the initial density matrix is calculated in the property .rho0() of the class ExperimentRunner , found in muspinsim/experiment.py .","title":"Preparing the initial state"},{"location":"theory_2/#time-evolution","text":"","title":"Time evolution"},{"location":"theory_2/#closed-system","text":"Time evolution for the density matrix of a closed quantum system is controlled by the Liouville-von Neumann equation that we've already seen: \\[ \\frac{\\partial \\rho}{\\partial t} = -\\frac{i}{\\hbar}[\\mathcal{H}, \\rho] \\] If we write the matrix quantities with indices (repeated indices imply summation) we can write this as a system of coupled differential equations for each individual coefficient: \\[ \\frac{\\partial \\rho_{ij}}{\\partial t} = -\\frac{i}{\\hbar}\\left(\\mathcal{H}_{ik}\\rho_{kj}-\\rho_{ik}\\mathcal{H}_{kj}\\right) \\] This gets significantly simpler when we express both the matrices in a basis in which the Hamiltonian is diagonal, and thus \\(\\mathcal{H}_{ij} = \\lambda_i\\delta_{ij}\\) : \\[ \\frac{\\partial \\rho_{ij}}{\\partial t} = -\\frac{i}{\\hbar}\\rho_{ij}\\left(\\lambda_i-\\lambda_j\\right) \\qquad \\implies \\qquad \\rho_{ij}(t) = e^{-\\frac{i}{\\hbar}\\left(\\lambda_i-\\lambda_j\\right) t}\\rho_{ij}(0) \\] This way we can see that the equations are completely decoupled. Coefficients on the diagonal of the density matrix don't change, while off-diagonal coefficients gain a phase factor at a constant rate that is dependent on the differences between the Hamiltonian eigenvalues. This method gives us the exact evolution of the system and perfectly preserves unitarity. The downside of it is that it requires a full diagonalization of the Hamiltonian. However, many spin systems that we are interested in are relatively small, and one single diagonalisation for each of them isn't a big deal. Cheaper, more approximate methods might be implemented in the future, but at the moment, MuSpinSim can easily carry out calculations on systems that don't exceed nine or ten spins on a laptop in a few minutes. For developers: time evolution of a system is handled by the .evolve() method of the Hamiltonian class.","title":"Closed system"},{"location":"theory_2/#integral-of-asymmetry","text":"In muon experiments we're usually interested in measuring the asymmetry of positron hits between the forward and back detectors in the experimental setup - namely, the polarisation of the muon along a certain axis, as it evolves in time. However, in some cases (like ALC experiments) what we actually care about is the integral of this asymmetry throughout a certain time interval. This could be trivially computed simply by computing the time evolution and then integrating numerically. However MuSpinSim in this case uses a different algorithm to perform the integral analytically, saving some unnecessary steps. The full derivation of the formula is detailed in this arXiv paper . The essence of it is that, if we have an operator \\(S\\) with matrix elements \\(s_{ij}\\) whose integral value we want to compute: $$ \\langle P \\rangle = \\int_0^\\infty \\langle S \\rangle(t) e^{-\\frac{t}{\\tau}} dt $$ where the integral is weighed with the decay process of the muon with lifetime \\(\\tau\\) , then we can define a new operator \\(P\\) with matrix elements: \\[ p_{ij} = \\frac{s_{ij}}{\\frac{1}{\\tau}-\\frac{i}{\\hbar}\\left(\\lambda_i-\\lambda_j\\right)} \\] and evaluating its expectation value on the initial state of the system will in a single pass return the value of the desired integral. For developers: integral expectation values are handled by the .integrate_decaying() method of the Hamiltonian class.","title":"Integral of asymmetry"},{"location":"theory_2/#open-system","text":"Systems described by the Liouville-von Neumann equation are closed; they conserve energy and evolve in a perfectly reversible way. This is sometimes not a good approximation, because in real life, the chunk of the sample that we're describing is of course only a small part of a much bigger system, fully coupled to it and interacting in a lot of ways. Since including an environment of hundreds or thousands of spins is not practical, a more common approach is to use a master equation that allows to describe irreversible evolution through some kind of energy exchange with environmental degrees of freedom. In MuSpinSim, the only such master equation that is supported is the simplest one, the Lindblad equation. It is an extension of the Liouville-von Neumann equation including dissipative terms: \\[ \\frac{\\partial \\rho}{\\partial t} = -\\frac{i}{\\hbar}[\\mathcal{H}, \\rho] + \\sum_{i=1}^{N^2-1}\\alpha_i\\left(L_i \\rho L_i^\\dagger - \\frac{1}{2}\\left\\{L_i^\\dagger L_i, \\rho\\right\\} \\right) \\] Here the \\(\\alpha_i\\) are coefficients that express the strength of the coupling with a certain degree of freedom, and the \\(L_i\\) are the so-called Lindblad or jump operators of the system, each connected to one coefficient. The curly braces denote the anticommutator of two matrices: \\(\\{A, B\\} = AB+BA\\) . This equation unfortunately does not have a neat solution in exponential form as the one seen above in the matrix formalism. It is however possible to find something very close to it by making a few small changes in the representation, namely, expressing the density matrix in what is called the Fock-Liouville space . An excellent and detailed explanation of this technique is given in this useful introductory paper by Daniel Manzano . The essence of it is that we \"straighten up\" the density matrix, writing all its elements in a single column vector. For example, a \\(4\\times 4\\) matrix can turn into a \\(16\\) elements column vector. It is then possible to write a matrix called the Lindbladian (that in the example will be \\(16 \\times 16\\) ) that operates on it exactly like a Hamiltonian does on a single wavefunction: \\[ \\frac{\\partial}{\\partial t} \\mid \\rho \\rangle\\rangle = \\mathcal{L} \\mid \\rho \\rangle\\rangle \\] and following from that, it is possible to integrate the equations as trivially as seen for the others by diagonalising the Lindbladian. Care must be taken though because unlike for the Hamiltonian, there is no guarantee that the Lindbladian is Hermitian, or for that matter, diagonalizable at all! This can potentially cause issues - however in my experience well-defined systems will be solvable without problems. In MuSpinSim, the only way dissipation can be included in a calculation is by putting an individual spin in contact with a thermal reserve. This is done by defining two jump operators for that spin, \\(S_+^i\\) and \\(S_-^i\\) , and the corresponding dissipation coefficients such that \\[ \\frac{\\alpha_+^i}{\\alpha_-^i} = \\exp\\left(-\\frac{\\hbar\\gamma |B|}{k_BT}\\right) \\] where \\(T\\) is the temperature of the system, and \\(\\hbar\\gamma|B|\\) is an approximation using only the Zeeman interaction of the energy gap between successive states of the spin. For \\(T < \\infty\\) , this is subject to the same limits as the choice of using only the Zeeman interaction to define the initial thermal state density matrix. In fact, the effect of these terms is to tend to drive the individual spin's state towards exactly that thermal state, adding or removing energy as needed and erasing coherences. For developers: the Lindbladian class is defined in muspinsim/lindbladian.py . It has .evolve() and .integrate_decaying() methods analogous to those of the Hamiltonian class. In the next section we will look specifically at the exact shape of the terms of the Hamiltonian (and when necessary, Lindbladian) used in MuSpinSim.","title":"Open system"}]}